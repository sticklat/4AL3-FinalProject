\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Progress Report:\\Monocular Object Distance Estimation Using YOLO Bounding Boxes}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

% Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.
\color{red} TODO:Update.\color{black}

Your report should have an introduction. You may largely copy the
motivation and problem description from your previous reports. Write about previous work
on your task or the most related task you can find. Note that you should include at least
7 references in your report. Did you discover anything in related work that influenced your
direction after the progress report was due? References should be integrated as part of the
discussion of the context of your work. They should not simply be listed as a set of relevant
papers with no explanation.

% Vehicles with ADAS (Advanced Driving Assistance Systems) capabilities need to able to know the distance to objects around them in order to navigate safely. While LIDAR
% sensors are often viewed as the gold standard for distance estimation, they are expensive and not always feasible for consumer vehicles.
% Another option is Radar, which is cheaper but has lower resolution and must contend with reflections and interference.
% Cameras are a cheaper alternative, but estimating depth from monocular images is a challenging problem. In this project, we aim to estimate
% the distance to objects using only monocular images and bounding boxes generated by the YOLO object detection algorithm.

% As a end goal, we hope to create a model that can accurately estimate the distance to objects so it can be usded in sensor fusion algorithms for
% autonomous vehicles. A combination of Radar and camera-based distance estimates is one way to help achieve a balance of cost and accuracy
% that is needed for consumer vehicles.


\section{Dataset}

\color{red} TODO:Update.\color{black}

You should describe the dataset properties and any preprocessing operations
you performed. If you are annotating data yourself, describe the annotation procedure you
developed and followed. Describe any changes you made to the dataset in a subsection if you
have changed your dataset since the progress report. Some of you found that your dataset
was too difficult or too easy. What did you have to change about your data? Did you have
to augment the data or move to a completely new source? If your dataset did not change, it
is reasonable to leave this mostly as is from the progress repor

% Our dataset is based on a processed subset of the \textit{nuScenes} autonomous driving dataset, which provides real-world 
% driving data collected from urban environments. Each scene includes synchronized camera images and accurate 3D bounding box 
% annotations with depth information derived from LIDAR sensors. These serve as the ground truth for our monocular distance 
% estimation model.

\section{Features and Inputs}

\color{red} TODO:Update.\color{black}
Describe your model inputs. What feature engineering or
representation learning was performed? Why did you add these features? Why does it make
sense to include them? Did you perform any feature selection or augmentation? One good
way to vary your experiments is to try different kinds of features as inputs. How did you
vary the features used for your experiments

% Each input sample to our model is derived from YOLOv8 detections and represented as a compact feature vector containing 
% geometric and categorical information. Specifically, we use normalized bounding box coordinates $(x_{\text{min}}, 
% y_{\text{min}}, x_{\text{max}}, y_{\text{max}})$, from which we compute the bounding box area and aspect ratio to capture 
% object scale and shape. The object class (e.g., car, pedestrian, truck) is encoded using a one-hot vector, allowing the 
% model to differentiate between object types without imposing ordinal relationships. All features are standardized before 
% training to ensure consistent scale and stable optimization. This combination of spatial and categorical inputs enables 
% the neural network to learn meaningful relationships between apparent object size and real-world distance, providing a 
% balance between efficiency and predictive accuracy.


\section{Implementation}

\color{red} TODO:Update.\color{black}

Describe the model implementation. You should have several mod-
els or versions of your model that you have run on your data. You should have a simple
baseline to compare it to (likely majority vote), and may have other baselines from related
work. Your model should outperform your simple baseline. For most of you, your model
should also outperform at least one other trained model baseline. If it does not outperform
this baseline and you expected it to, why doesn’t it? You should be able to provide an expla-
nation. This explanation can also be part of your error analysis section. You do not have to
outperform all models from previous work, but you should have an approach you implement
for comparison. What was your loss function? Describe the optimization technique. If you
implemented a complex model with many parts, you should consider providing ablations.
This means different experiments where you only include one feature at a time so you can
tell what feature is contributing more to the performance that you see.

% Describe your model and implementation here. Refer to item 4. This may take around a page.
% Our current implementation is a simplfied object based distance estimator. It uses YOLOv8\cite{yolov8_ultralytics}
% to generate bounding box coordinates and object classification, then passing this data to our model. We found that
% for this set of input features a 8 layer neural netowork with 128 neurons per layer and ELU activation worked best.

% To establish a performance baseline, we compared our model design to the Disnet architecture~\cite{Haseeb2018DisNetAN}, 
% which achieved a Mean Absolute Error (MAE) of approximately 2.0 using similar object-based features. This provides 
% a meaningful benchmark for our current implementation, as it allows us to directly assess improvements in model accuracy 
% and efficiency relative to prior work. The updated version of Disnet used in~\cite{Franke2021} achieved better performance, but
% it relied on an optimized YOLO model and more complex training strategies. The final RMSE results were as follows:
\begin{table}[h]
  \centering
  \captionsetup{justification=centering}
  \begin{tabular}{cccc}
    \toprule
    \textbf{RMSE} & Car & Person & \textbf{Total} \\
    \midrule
    \textbf{Initial Disnet} & 38.2  & 38.21 & 38.20 \\
    \textbf{Disnet}         & 13.72 & 10.52 & 10.90 \\
    \bottomrule
  \end{tabular}
  \caption{RMSE comparison between Initial DisNet~\cite{Haseeb2018DisNetAN} and DisNet~\cite{Franke2021}}
\end{table}


\section{Evaluation}

\color{red} TODO:Update.\color{black}

Describe the evaluation strategy. What are your train/validation/test
splits (size and label distributions)? Are you using cross-validation? What metrics are you
using for evaluation? Did you find that your metrics from the progress report stage were
adequate?

% We evaluate our model using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the predicted and 
% ground-truth object distances. The ground-truth annotations were derived from the normalized dataset provided in 
% \texttt{annotations\_with\_id.csv}, and predictions were generated through our neural network model using YOLOv8-based 
% bounding box features. After running the evaluation pipeline, the model achieved an MAE of approximately 1.52 and an RMSE 
% of 2.18, indicating strong predictive performance given the simplicity of the input features. Qualitatively, most predictions 
% fall within $\pm10\%$ of the true distance, demonstrating the model’s ability to infer depth information from spatial geometry 
% and class cues alone. 

% For comparison, the original Disnet implementation reported a Mean Absolute Error around 2.0 after convergence across networks 
% with 3–10 hidden layers. Our model achieved a lower MAE of 1.52, suggesting improved distance estimation accuracy despite using 
% a simpler architecture and more compact input representation. These results validate the feasibility of a lightweight, 
% bounding-box-based approach to monocular distance estimation and provide a strong baseline for future experiments, such as 
% fine-tuning network hyperparameters or expanding the training dataset for better generalization.

\section{Progress}

\color{red} TODO:Fill in this section.\color{black}

Reflect on your plan from your progress report. Did you follow-through on
your plan? Did your plan change course? Why

% Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.
% Our plans for the remainder of the project include several key improvements, the first of which are to how
% we preprocess and engineer features from the YOLO bounding box outputs. Specifically, we plan to add more advanced
% features that better capture the relationship between bounding box geometry and real-world distance. This might include
% features such as the object rotation angle and occlusion level, which could provide additional context for distance estimation.
% We are also considering changing the scale of bounding boxes to be relative to the image size, as well as scaling bounding boxes based
% on known object dimensions (e.g., average car width). These enhancements should provide the model with more
% informative inputs so the model able to perform in a widder range of scenarios.

% In regards to model architecture, we plan to experiment with other model types beyond our current neural network. This
% could include decision trees or ensemble methods like random forests or gradient boosting machines talked about in class.
% However given the success of the current model and what has been done in related works, it is likely that this
% kind of estimiation is best suited to simpler neural networks.

% Finally, we plan to conduct a more thorough hyperparameter optimization process through random search to fine-tune
% the learning rate, batch size, model shape, activation functions, and regularization techniques. This should help
% us identify the optimal configuration for our model and further improve performance. 


\section{Error Analysis}

\color{red} TODO:Fill in this section.\color{black}

Describe how you systematically examine the errors your model
makes and provide supporting figures, stats, examples (e.g., confusion matrices, qualitative
sample of test cases with high error margins, etc). What does your model appear to be good
at? What does it seem to be bad at? How does the performance of your models differ? What
patterns do you notice in the errors your model seems to make? What do you think you
could do to specifically address those issues if you were to continue working on this model?

\section*{Team Contributions}

Thomas: (\verb|src/estimator/*|)
\begin{itemize}
  \item Feature import to tensor
  \item NN model
  \item Hyperoptimization
  \item Training function
\end{itemize}

Chengze: (\verb|src/Loss/*|)
\begin{itemize}
  \item Implemented model evaluation and loss computation scripts
  \item Generated and analyzed MAE/RMSE metrics for performance comparison
  \item Integrated YOLOv8 bounding box outputs into the distance regression pipeline
  \item Produced result reports (\verb|model_preds.csv|, \verb|metrics_model.csv|) for validation and\\ benchmarking
\end{itemize}

Yicheng: (\verb|src/objectDetection/*|)
\begin{itemize}
  \item object detection with existing model
  \item generating bounding box with classification
\end{itemize}




% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
