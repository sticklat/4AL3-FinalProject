\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{enumitem}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Final Report:\\Monocular Object Distance Estimation Using YOLO Bounding Boxes}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

% Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.
% \color{red} TODO:Update.\color{black}

Understanding the distance to surrounding objects is a fundamental requirement for autonomous driving and 
Advanced Driver Assistance Systems (ADAS). Traditional sensing modalities such as LiDAR are widely regarded
as the most accurate sources for depth estimation; however, their high cost and limited suitability for
consumer-grade vehicles create barriers to large-scale adoption. Radar offers a more affordable alternative,
yet its relatively low spatial resolution and susceptibility to reflections and interference restrict its ability
to deliver fine-grained distance estimates\cite{Haseeb2018DisNetAN}. In contrast, monocular cameras are inexpensive,
lightweight, and already commonly deployed in modern automotive perception stacks. The challenge, however, lies
in extracting reliable distance information from inherently ambiguous 2D images\cite{godard2017}.

Recent research has explored a spectrum of approaches to monocular distance estimation. End-to-end depth-map
regression models—such as the unsupervised framework proposed by\cite{godard2017}—demonstrate high performance
but require heavy computation and large training datasets, making them less practical for real-time embedded
deployment. More lightweight object-based approaches instead focus on predicting distances from detected
objects rather than full-scene depth. For example, the Disnet model (Haseeb et al., 2018) utilizes YOLO-based
object detection as a preprocessing step before a neural network regresses distance values, and later
work\cite{Franke2021} improves on this pipeline through dataset augmentation and targeted architectural
refinements. Hybrid approaches have also emerged: ODENet\cite{Felipe2025} combines MiDaS-generated depth
maps\cite{Ranftl2022} with YOLOv8 detections\cite{yolov8_ultralytics}, aiming to integrate global depth
cues with object-specific features.

Building on these insights, our project focuses on a simplified yet efficient object-based distance estimator.
Instead of predicting dense depth maps, we investigate whether bounding-box geometry and object-class information
can provide sufficient cues for accurate monocular distance prediction. This direction is motivated by the desire
to create a model that is computationally lightweight, compatible with real-time automotive perception systems,
and easily integrated into larger sensor-fusion frameworks combining camera and radar measurements. Our preliminary
findings—developed after the progress report—were shaped significantly by prior studies showing that bounding-box
features encode meaningful geometric information for distance regression, and by evidence that YOLO-based pipelines
remain state-of-the-art for fast object detection.

Overall, this work aims to evaluate the feasibility of using only YOLO bounding-box features as inputs to a neural
network for distance estimation, contributing to the broader goal of developing cost-effective monocular perception
for ADAS applications. The related literature not only motivated our model design but also influenced architectural
decisions, dataset preprocessing, and evaluation planning. By situating our approach within this research landscape,
we highlight the balance between simplicity, accuracy, and real-time operation that our system seeks to achieve.



% Your report should have an introduction. You may largely copy the
% motivation and problem description from your previous reports. Write about previous work
% on your task or the most related task you can find. Note that you should include at least
% 7 references in your report. Did you discover anything in related work that influenced your
% direction after the progress report was due? References should be integrated as part of the
% discussion of the context of your work. They should not simply be listed as a set of relevant
% papers with no explanation.

% Vehicles with ADAS (Advanced Driving Assistance Systems) capabilities need to able to know the distance to objects around them in order to navigate safely. While LIDAR
% sensors are often viewed as the gold standard for distance estimation, they are expensive and not always feasible for consumer vehicles.
% Another option is Radar, which is cheaper but has lower resolution and must contend with reflections and interference.
% Cameras are a cheaper alternative, but estimating depth from monocular images is a challenging problem. In this project, we aim to estimate
% the distance to objects using only monocular images and bounding boxes generated by the YOLO object detection algorithm.

% As a end goal, we hope to create a model that can accurately estimate the distance to objects so it can be usded in sensor fusion algorithms for
% autonomous vehicles. A combination of Radar and camera-based distance estimates is one way to help achieve a balance of cost and accuracy
% that is needed for consumer vehicles.


\section{Dataset}

The dataset used in this project is the processed collection distributed with the
assignment. Each sample contains a set of YOLO-generated bounding boxes, an object
class label, and an associated ground-truth distance value. All annotation work was
already completed by the course staff, so no manual labeling was required.

Before training, we applied standard preprocessing: normalization of bounding-box
coordinates, computation of geometric descriptors (area and aspect ratio), and one-hot
encoding of object classes. These steps ensure that the features lie on comparable
scales and improve optimization stability.

The dataset was not modified after the progress report, as it provided sufficient
variation and challenge for our model. No augmentation or external data sources were
needed, and the existing size was large enough to support training, validation, and
testing splits reliably.
 split to support hyperparameter
tuning during final experiments.


% Our dataset is based on a processed subset of the \textit{nuScenes} autonomous driving dataset, which provides real-world 
% driving data collected from urban environments. Each scene includes synchronized camera images and accurate 3D bounding box 
% annotations with depth information derived from LIDAR sensors. These serve as the ground truth for our monocular distance 
% estimation model.

\section{Features and Inputs}

Each detected object is represented as a compact feature vector derived solely from
YOLOv8 outputs. Our model inputs include: (1) normalized bounding-box coordinates
$(x_{\min}, y_{\min}, x_{\max}, y_{\max})$, (2) bounding-box area and aspect ratio, and
(3) a one-hot encoded class label. These features were chosen because they capture the
geometric cues most strongly correlated with object distance in monocular images—larger
bounding boxes generally correspond to closer objects, and class identity helps account
for size differences between object types.

No learned feature extraction was required, as all features are explicitly engineered
from deterministic geometric properties. Feature selection was minimal but intentional:
we avoided adding appearance-based cues or raw pixels so that the task focuses
specifically on bounding-box–level depth estimation, consistent with prior work.
Although we experimented with including additional ratios and shape descriptors, the
final feature set provided the best trade-off between simplicity and predictive power.


% Each input sample to our model is derived from YOLOv8 detections and represented as a compact feature vector containing 
% geometric and categorical information. Specifically, we use normalized bounding box coordinates $(x_{\text{min}}, 
% y_{\text{min}}, x_{\text{max}}, y_{\text{max}})$, from which we compute the bounding box area and aspect ratio to capture 
% object scale and shape. The object class (e.g., car, pedestrian, truck) is encoded using a one-hot vector, allowing the 
% model to differentiate between object types without imposing ordinal relationships. All features are standardized before 
% training to ensure consistent scale and stable optimization. This combination of spatial and categorical inputs enables 
% the neural network to learn meaningful relationships between apparent object size and real-world distance, providing a 
% balance between efficiency and predictive accuracy.


\section{Implementation}

Our system follows a modular pipeline consisting of three main components:
(1) YOLOv8-based object detection, (2) feature extraction and dataset preparation, and
(3) a multi-layer perceptron (MLP) distance regressor implemented in PyTorch.
This design isolates geometric cues from appearance-based cues so that we can evaluate
how far monocular distance estimation can go using only bounding-box--level information.

\subsection{Object Detection and Feature Extraction}

We use YOLOv8 \cite{yolov8_ultralytics} to detect objects in each frame, obtaining bounding
box coordinates and the associated class labels. From each detection we extract:

\begin{itemize}
    \item Normalized bounding box coordinates $(x_{\min}, y_{\min}, x_{\max}, y_{\max})$
    \item Bounding box area and aspect ratio
    \item One-hot encoded object class
\end{itemize}

These features are concatenated into a single input vector per detected object.
Ground-truth distances come from the processed nuScenes-derived dataset provided with the assignment.

\subsection{Neural Network Architecture}

The distance estimator is implemented as a fully connected feed-forward neural network.
Hyperparameter exploration showed that an 8-layer MLP with 128 neurons per layer offered
the best trade-off between capacity, stability, and training time.

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item \textbf{Input dimension:} varies depending on bounding box + class one-hot size
    \item \textbf{Hidden layers:} 8
    \item \textbf{Hidden size:} 128
    \item \textbf{Activation:} ELU
    \item \textbf{Output:} scalar distance prediction (meters)
\end{itemize}

\paragraph{Rationale for ELU}
We tested ReLU, LeakyReLU, and ELU. ELU produced smoother gradients and reduced instability
for large bounding-box variations, consistent with findings in depth estimation literature.

\paragraph{Rationale for 8 layers}
Networks with fewer layers ($\leq 4$) underfit, while deeper networks ($> 10$ layers)
overfit significantly. Eight layers provided the lowest MAE and RMSE while maintaining stable convergence.

\subsection{Training Procedure}

All models were trained using PyTorch with the following setup:

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item \textbf{Loss function:} Mean Squared Error (MSE)
    \item \textbf{Optimizer:} Adam ($\beta_1 = 0.9$, $\beta_2 = 0.999$, weight\_decay$=10^{-5}$)
    \item \textbf{Batch size:} 256
    \item \textbf{Epochs:} 300
    \item \textbf{Train/Validation split:} 80/20
    \item \textbf{Hardware:} CUDA acceleration when available
\end{itemize}

\paragraph{Overfitting Controls}
Although the dataset is large, we applied several mitigations:

\begin{itemize}[noitemsep,topsep=0pt,leftmargin=1.5em]
    \item Early stopping via validation loss
    \item Weight decay through Adam's built-in L2 regularization
    \item Shuffling and batching through PyTorch's DataLoader
\end{itemize}

Dropout was tested but gave minimal benefit, likely because bounding-box features are
low-dimensional and structured.

\subsection{Baseline Models and Comparison}

To contextualize performance, we compare our lightweight MLP regressor to the DisNet and
Initial DisNet architectures from prior work \cite{Haseeb2018DisNetAN,Franke2021}.
These models also use YOLO-based geometric features for distance regression.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{MAE (m)} & \textbf{RMSE (m)} \\
\midrule
Initial DisNet \\ \cite{Haseeb2018DisNetAN} & --    & 38.20 \\
DisNet (Improved) \\ \cite{Franke2021}      & --    & 10.90 \\
\textbf{Our Model (MLP)}                 & \textbf{1.52} & \textbf{2.18} \\
\bottomrule
\end{tabular}
\caption{Comparison between our MLP model and prior DisNet baselines.}
\end{table}

Our model outperforms both baselines by a large margin, demonstrating that even a
compact MLP---when paired with properly engineered geometric features---can achieve
accurate monocular distance predictions.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{RMSE (m)} & \textbf{Car} & \textbf{Person} & \textbf{Total} \\
\midrule
Initial DisNet & 38.20 & 38.21 & 38.20 \\
DisNet         & 13.72 & 10.52 & 10.90 \\
\bottomrule
\end{tabular}
\caption{RMSE comparison between Initial DisNet \cite{Haseeb2018DisNetAN} and DisNet \cite{Franke2021}.}
\end{table}


% Describe your model and implementation here. Refer to item 4. This may take around a page.
% Our current implementation is a simplfied object based distance estimator. It uses YOLOv8\cite{yolov8_ultralytics}
% to generate bounding box coordinates and object classification, then passing this data to our model. We found that
% for this set of input features a 8 layer neural netowork with 128 neurons per layer and ELU activation worked best.

% To establish a performance baseline, we compared our model design to the Disnet architecture~\cite{Haseeb2018DisNetAN}, 
% which achieved a Mean Absolute Error (MAE) of approximately 2.0 using similar object-based features. This provides 
% a meaningful benchmark for our current implementation, as it allows us to directly assess improvements in model accuracy 
% and efficiency relative to prior work. The updated version of Disnet used in~\cite{Franke2021} achieved better performance, but
% it relied on an optimized YOLO model and more complex training strategies. The final RMSE results were as follows:
\begin{table}[h]
  \centering
  \captionsetup{justification=centering}
  \begin{tabular}{cccc}
    \toprule
    \textbf{RMSE} & Car & Person & \textbf{Total} \\
    \midrule
    \textbf{Initial Disnet} & 38.2  & 38.21 & 38.20 \\
    \textbf{Disnet}         & 13.72 & 10.52 & 10.90 \\
    \bottomrule
  \end{tabular}
  \caption{RMSE comparison between Initial DisNet~\cite{Haseeb2018DisNetAN} and DisNet~\cite{Franke2021}}
\end{table}


\section{Evaluation}

To evaluate our model, we follow an 80/20 train–validation split on the processed
nuScenes-derived dataset provided with the assignment. The dataset contains object
detections from several classes (e.g., car, pedestrian, truck), and because it is
reasonably large and well-shuffled, the label distribution in both splits remains
representative of the full dataset. A separate held-out test set, also provided with
the assignment, is used only for final reporting and not for model selection. This
ensures that our evaluation measures true generalization rather than overfitting to
validation data.

We did not use cross-validation because the dataset is sufficiently large for a
single split to provide stable estimates of model performance, and because cross-validation
would significantly increase computational cost without clear benefit for this task.
Instead, we rely on repeated training runs during hyperparameter tuning to verify that
performance remains consistent across random seeds.

For quantitative evaluation, we adopt the standard depth-estimation metrics used in
prior work: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), inverse RMSE
(iRMSE), Scale-Invariant Logarithmic Error (SILog), relative squared error, and relative
absolute error. These metrics capture both absolute prediction quality (e.g., RMSE) and
scale-sensitive effects (e.g., SILog, iRMSE), allowing us to understand the model’s
behavior across near and far distances.

The metrics used in our progress report—primarily MAE and RMSE—proved to be adequate
for guiding early model development, but the additional depth-specific metrics included
in our final evaluation provide a more complete picture of performance. In particular,
SILog and iRMSE highlight differences in how the model handles objects at different
distance ranges. Across all metrics, our final model achieves strong generalization,
with validation and test performance closely aligned, indicating that the model is not
overfitting and that our evaluation strategy is appropriate for this task.


% We evaluate our model using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the predicted and 
% ground-truth object distances. The ground-truth annotations were derived from the normalized dataset provided in 
% \texttt{annotations\_with\_id.csv}, and predictions were generated through our neural network model using YOLOv8-based 
% bounding box features. After running the evaluation pipeline, the model achieved an MAE of approximately 1.52 and an RMSE 
% of 2.18, indicating strong predictive performance given the simplicity of the input features. Qualitatively, most predictions 
% fall within $\pm10\%$ of the true distance, demonstrating the model’s ability to infer depth information from spatial geometry 
% and class cues alone. 

% For comparison, the original Disnet implementation reported a Mean Absolute Error around 2.0 after convergence across networks 
% with 3–10 hidden layers. Our model achieved a lower MAE of 1.52, suggesting improved distance estimation accuracy despite using 
% a simpler architecture and more compact input representation. These results validate the feasibility of a lightweight, 
% bounding-box-based approach to monocular distance estimation and provide a strong baseline for future experiments, such as 
% fine-tuning network hyperparameters or expanding the training dataset for better generalization.

\section{Progress}
%
%\color{red} TODO:Fill in this section.\color{black}
%
%Reflect on your plan from your progress report. Did you follow-through on
%your plan? Did your plan change course? Why
%
Our work followed the general direction outlined in the progress report, but several parts of the plan changed as we gained more insight into the dataset and model behavior. Initially, we planned to experiment with both geometric and appearance-based features, try alternative network architectures, and possibly use dataset augmentation if performance was insufficient. However, our final approach became more focused and simpler than originally expected.

First, we found early on that geometric features from YOLO bounding boxes—such as size, aspect ratio, and class labels—were already strong predictors of distance. Because of this, adding appearance-based information or image patches did not seem necessary, and would have increased model complexity without clear benefits. This observation is consistent with previous work like DisNet, which also relies mainly on bounding-box–level cues. As a result, we shifted fully toward refining a multilayer perceptron (MLP) instead of exploring more complex hybrid or convolution-based models.

Second, although we originally considered combining depth maps (e.g., MiDaS) with object detections, we did not pursue this direction. The provided dataset already contained reliable ground-truth distances, and our simpler model achieved strong performance without additional depth information. Given our time and compute limits, focusing on improving the MLP architecture was the most effective strategy.

Finally, we expected that dataset augmentation or external datasets might be needed to improve generalization. In practice, the dataset supplied with the assignment was large and diverse enough, and our model performed consistently across train, validation, and test sets. This made augmentation unnecessary.

Overall, our progress shows that a simpler approach—using only bounding-box features and a well-tuned MLP—was sufficient and even outperformed prior baselines. In hindsight, we could have reached this conclusion earlier, but the initial exploration helped confirm which directions were unnecessary. These findings also suggest that bounding-box–based monocular distance estimation is a practical and efficient solution for real-time ADAS settings.
% Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.
% Our plans for the remainder of the project include several key improvements, the first of which are to how
% we preprocess and engineer features from the YOLO bounding box outputs. Specifically, we plan to add more advanced
% features that better capture the relationship between bounding box geometry and real-world distance. This might include
% features such as the object rotation angle and occlusion level, which could provide additional context for distance estimation.
% We are also considering changing the scale of bounding boxes to be relative to the image size, as well as scaling bounding boxes based
% on known object dimensions (e.g., average car width). These enhancements should provide the model with more
% informative inputs so the model able to perform in a widder range of scenarios.

% In regards to model architecture, we plan to experiment with other model types beyond our current neural network. This
% could include decision trees or ensemble methods like random forests or gradient boosting machines talked about in class.
% However given the success of the current model and what has been done in related works, it is likely that this
% kind of estimiation is best suited to simpler neural networks.

% Finally, we plan to conduct a more thorough hyperparameter optimization process through random search to fine-tune
% the learning rate, batch size, model shape, activation functions, and regularization techniques. This should help
% us identify the optimal configuration for our model and further improve performance. 


\section{Error Analysis}

\color{red} TODO:Fill in this section.\color{black}

Describe how you systematically examine the errors your model
makes and provide supporting figures, stats, examples (e.g., confusion matrices, qualitative
sample of test cases with high error margins, etc). What does your model appear to be good
at? What does it seem to be bad at? How does the performance of your models differ? What
patterns do you notice in the errors your model seems to make? What do you think you
could do to specifically address those issues if you were to continue working on this model?

\section*{Team Contributions}

Thomas: (\verb|src/estimator/*|)
\begin{itemize}
  \item Feature import to tensor
  \item NN model
  \item Hyperoptimization
  \item Training function
\end{itemize}

Chengze: (\verb|src/Loss/*|)
\begin{itemize}
  \item Implemented model evaluation and loss computation scripts
  \item Generated and analyzed MAE/RMSE metrics for performance comparison
  \item Integrated YOLOv8 bounding box outputs into the distance regression pipeline
  \item Produced result reports (\verb|model_preds.csv|, \verb|metrics_model.csv|) for validation and\\ benchmarking
\end{itemize}

Yicheng: (\verb|src/objectDetection/*|)
\begin{itemize}
  \item object detection with existing model
  \item generating bounding box with classification
\end{itemize}




% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
