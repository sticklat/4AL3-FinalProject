\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Progress Report:\\Monocular Object Distance Estimation Using YOLO Bounding Boxes}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

% Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.

Vehicles with ADAS (Advanced Driving Assistance Systems) capabilities need to able to know the distance to objects around them in order to navigate safely. While LIDAR
sensors are often viewed as the gold standard for distance estimation, they are expensive and not always feasible for consumer vehicles.
Another option is Radar, which is cheaper but has lower resolution and must contend with reflections and interference.
Cameras are a cheaper alternative, but estimating depth from monocular images is a challenging problem. In this project, we aim to estimate
the distance to objects using only monocular images and bounding boxes generated by the YOLO object detection algorithm.

As a end goal, we hope to create a model that can accurately estimate the distance to objects so it can be usded in sensor fusion algorithms for
autonomous vehicles. A combination of Radar and camera-based distance estimates is one way to help achieve a balance of cost and accuracy
that is needed for consumer vehicles.


\section{Related Work}

% Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.

We surveyed a number of related works in the area of monocular distance estimation. Many works focus on
end-to-end deep learning approaches that take in raw images and full output depth maps.
For example, \cite{godard2017} proposed an unsupervised learning approach for depth estimation using
rectified monocular images. However, these methods are theoretically and computationally
more complex than object based distance estimation. While depth maps are required for a camera only system, 
we theorize it is possible to achieve good distance estimation performance using only object bounding boxes.
This would allow for a simpler and more efficient model that could be more easily integrated into existing
autonomous vehicle systems.

Examples that we found that that took simpler approaches to this issue include Disnet which uses 
a YOLO pre-pipline to handle object detection, passing the object data from YOLO to a Multi Hidden-Layer Neural
Network to estimate the distance of this object. Disnet was trained using supervised techniques on 3d laser scanner (LIDAR)
measurements as ground truth.\cite{Haseeb2018DisNetAN}. Others such as\cite{Franke2021} have build off Disnet,
expanding it with optimized YOLO models.

More recently some such as \cite{Felipe2025} have created a new approach as a combination of depth-map and
object based distance esimation: Object Distance Estimation Network (ODENet). This approach combines a depth map generated
using MiDaS\cite{Ranftl2022} and object image detection from YOLOv8\cite{yolov8_ultralytics} with a multilayer
perceptron.


\section{Dataset}

Our dataset is based on a processed subset of the \textit{nuScenes} autonomous driving dataset, which provides real-world 
driving data collected from urban environments. Each scene includes synchronized camera images and accurate 3D bounding box 
annotations with depth information derived from LIDAR sensors. These serve as the ground truth for our monocular distance 
estimation model.

\subsection{Preprocessing}

We focused on the front camera view to align with a typical monocular setup. Each image was first passed through the 
YOLOv8 \cite{yolov8_ultralytics} object detection model to extract bounding boxes, object classes, and confidence scores. 
We retained only detections with a confidence score greater than 0.5 to reduce noise.

The processed detections were then linked with their corresponding ground-truth distances using the provided annotation 
file:
\begin{center}
\texttt{src/loss/data/annotations\_with\_id.csv}
\end{center}

Each annotation includes the following fields:
(x\_min, y\_min, x\_max, y\_max, class, z\_true, row\_id)

where $(x_{\text{min}}, y_{\text{min}}, x_{\text{max}}, y_{\text{max}})$ denote the bounding box coordinates, \textit{class} 
specifies the object type (e.g., car, pedestrian, truck), and $\text{z}_{\text{true}}$ is the ground-truth distance from the 
camera in meters.

All preprocessed data are consolidated into a PyTorch dataset file:
\begin{center}
\texttt{vehicle\_dataset.pt}
\end{center}
This file is used as the primary input during model training and evaluation.

\subsection{Annotation and Splitting}

Each record in the annotation file corresponds to one detected object. The dataset is split into training, validation, and 
testing subsets with a 70:15:15 ratio to ensure balanced evaluation. The validation and test splits include objects at 
varying distances and classes to test the model’s ability to generalize across object types and camera perspectives.

\subsection{Dataset Statistics}

The processed dataset contains approximately 40,570 labeled samples. The distribution of distances ranges from 2 to 120 
meters, with a median of about 35 meters. The dataset is class-imbalanced, as vehicles dominate the object count, followed 
by pedestrians and traffic signs.

\subsection{Generated Predictions}

After training, the model’s predictions are stored in:
\begin{center}
\texttt{reports/model\_preds.csv}
\end{center}
This file contains predicted distances ($z_{\text{pred}}$) and ground-truth values ($z_{\text{true}}$), which are later 
evaluated using mean absolute error (MAE) and root mean square error (RMSE) metrics to assess model performance.


\section{Features}

Each input sample to our model is derived from YOLOv8 detections and represented as a compact feature vector containing 
geometric and categorical information. Specifically, we use normalized bounding box coordinates $(x_{\text{min}}, 
y_{\text{min}}, x_{\text{max}}, y_{\text{max}})$, from which we compute the bounding box area and aspect ratio to capture 
object scale and shape. The object class (e.g., car, pedestrian, truck) is encoded using a one-hot vector, allowing the 
model to differentiate between object types without imposing ordinal relationships. All features are standardized before 
training to ensure consistent scale and stable optimization. This combination of spatial and categorical inputs enables 
the neural network to learn meaningful relationships between apparent object size and real-world distance, providing a 
balance between efficiency and predictive accuracy.


\section{Implementation}

% Describe your model and implementation here. Refer to item 4. This may take around a page.
Our current implementation is a simplfied object based distance estimator. It uses YOLOv8\cite{yolov8_ultralytics}
to generate bounding box coordinates and object classification, then passing this data to our model. We found that
for this set of input features a 8 layer neural netowork with 128 neurons per layer and ELU activation worked best.

To establish a performance baseline, we compared our model design to the Disnet architecture~\cite{Haseeb2018DisNetAN}, 
which achieved a Mean Absolute Error (MAE) of approximately 2.0 using similar object-based features. This provides 
a meaningful benchmark for our current implementation, as it allows us to directly assess improvements in model accuracy 
and efficiency relative to prior work. The updated version of Disnet used in~\cite{Franke2021} achieved better performance, but
it relied on an optimized YOLO model and more complex training strategies. The final RMSE results were as follows:
\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{RMSE} & Car & Person & \textbf{Total} \\
    \hline
    \textbf{Initial Disnet} & 38.2\% & 38.21\% & 38.20\% \\
    \hline
    \textbf{Disnet} & 13.72\% & 10.52\% & 10.90\% \\
    \hline
  \end{tabular}
  \caption{RMSE comparison between Initial DisNet~\cite{Haseeb2018DisNetAN} and DisNet~\cite{Franke2021}}
\end{table}

\section{Results and Evaluation}

We evaluate our model using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the predicted and 
ground-truth object distances. The ground-truth annotations were derived from the normalized dataset provided in 
\texttt{annotations\_with\_id.csv}, and predictions were generated through our neural network model using YOLOv8-based 
bounding box features. After running the evaluation pipeline, the model achieved an MAE of approximately 1.52 and an RMSE 
of 2.18, indicating strong predictive performance given the simplicity of the input features. Qualitatively, most predictions 
fall within $\pm10\%$ of the true distance, demonstrating the model’s ability to infer depth information from spatial geometry 
and class cues alone. 

For comparison, the original Disnet implementation reported a Mean Absolute Error around 2.0 after convergence across networks 
with 3–10 hidden layers. Our model achieved a lower MAE of 1.52, suggesting improved distance estimation accuracy despite using 
a simpler architecture and more compact input representation. These results validate the feasibility of a lightweight, 
bounding-box-based approach to monocular distance estimation and provide a strong baseline for future experiments, such as 
fine-tuning network hyperparameters or expanding the training dataset for better generalization.

\section{Feedback and Plans}

% Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.
Our plans for the remainder of the project include several key improvements, the first of which are to how
we preprocess and engineer features from the YOLO bounding box outputs. Specifically, we plan to add more advanced
features that better capture the relationship between bounding box geometry and real-world distance. This might include
features such as the object rotation angle and occlusion level, which could provide additional context for distance estimation.
We are also considering changing the scale of bounding boxes to be relative to the image size, as well as scaling bounding boxes based
on known object dimensions (e.g., average car width). These enhancements should provide the model with more
informative inputs so the model able to perform in a widder range of scenarios.

In regards to model architecture, we plan to experiment with other model types beyond our current neural network. This
could include decision trees or ensemble methods like random forests or gradient boosting machines talked about in class.
However given the success of the current model and what has been done in related works, it is likely that this
kind of estimiation is best suited to simpler neural networks.

Finally, we plan to conduct a more thorough hyperparameter optimization process through random search to fine-tune
the learning rate, batch size, model shape, activation functions, and regularization techniques. This should help
us identify the optimal configuration for our model and further improve performance. 
% \section{Template Notes}

% You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

% % \subsection{Tables and figures}

% % See Table~\ref{citation-guide} for an example of a table and its caption.
% % See Figure~\ref{fig:experiments} for an example of a figure and its caption.


% % \begin{figure}[t]
% %   \includegraphics[width=\columnwidth]{example-image-golden}
% %   \caption{A figure with a caption that runs for more than one line.
% %     Example image is usually available through the \texttt{mwe} package
% %     without even mentioning it in the preamble.}
% %   \label{fig:experiments}
% % \end{figure}

% % \begin{figure*}[t]
% %   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
% %   \includegraphics[width=0.48\linewidth]{example-image-b}
% %   \caption {A minimal working example to demonstrate how to place
% %     two images side-by-side.}
% % \end{figure*}


% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.
% This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Thomas: (\verb|src/estimator/*|)
\begin{itemize}
  \item Feature import to tensor
  \item NN model
  \item Hyperoptimization
  \item Training function
\end{itemize}

Chengze: (\verb|src/Loss/*|)
\begin{itemize}
  \item Implemented model evaluation and loss computation scripts
  \item Generated and analyzed MAE/RMSE metrics for performance comparison
  \item Integrated YOLOv8 bounding box outputs into the distance regression pipeline
  \item Produced result reports (\verb|model_preds.csv|, \verb|metrics_model.csv|) for validation and benchmarking
\end{itemize}

Yicheng: (\verb|src/objectDetection/*|)
\begin{itemize}
  \item object detection with existing model
  \item generating bounding box with classification
\end{itemize}




% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
