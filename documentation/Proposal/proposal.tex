\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{url}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Project Proposal:\\My Group's Project Name: BoxBoxBox}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Overview}

Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.

\section{Task Title and Overview}

Task title and overview, including the significance and what makes it challenging

\section{Dataset}

For this project, we will use the KITTI Depth Prediction dataset, part of the publicly availableK
ITTI Vision Benchmark Suite (\url{https://www.cvlibs.net/datasets/kitti/eval_depth_all.php}). The dataset 
was collected by the Karlsruhe Institute of Technology and the Toyota Technological Institute using 
was collected by the Karlsruhe Institute of Technology and the Toyota Technological Institute using 
a stereo camera system and a Velodyne HDL-64E LiDAR sensor mounted on a moving vehicle. Each sample 
in the dataset contains a RGB image captured by the cameras and a corresponding dense depth map 
generated from LiDAR measurements. The RGB image serves as the input (X) for our model, while the 
depth map acts as the label (Y), representing the ground-truth distance between each pixel in the 
image and the camera. These depth maps are provided as part of the dataset and were automatically 
generated by projecting LiDAR 3D points into the 2D camera image plane, eliminating the need for 
manual labeling.

Since the KITTI dataset is open-source and publicly available, no scraping or API access is required, 
and all data usage complies with KITTI's academic research license and terms of service. The dataset 
also includes metadata such as sensor calibration parameters, camera poses, timestamps, and 
environmental context, which enhance reproducibility and model alignment. As the full dataset consists 
of over 93,000 stereo image pairs, we plan to use a 10-20\% subset to reduce computational load while 
maintaining coverage across diverse environments (urban, rural, and highway). The dataset's high-quality 
annotations and real-world depth data make it ideal for supervised training of our vision-based obstacle 
detection and distance estimation model for autonomous drones. Unlike datasets without labels—which 
require manual annotation of objects or distances—KITTI provides pre-labeled, LiDAR-verified ground-truth 
depth values, ensuring accuracy and efficiency in the training process.

\section{Proposed Solution}
We plan to solve this problem by using a combination of object detection and regression models. However we will
first begin with prelabelled images to remove the object detection work, then once we have proven that our model
works with this prelabelled data we can add the object detection into our pipeline. For this we would use an
object detection CNN like YOLO11 to identify objects in the image and their bounding boxes, then use our model
to estimate the distance to each object based on the bounding box coordinates and the object's class\cite{Franke_Gopinath_Reddy_Ristic-Durrant_Michels_2021}.
If sucessful, this will allow us to estimate the distance to multiple objects in an image.

To build upon this first step and further improve the accuracy of our model we could add orientation data to
our feature set. This would allow us to account for the 3d orientation of objects in the image, which would
help us to better estimate the distance to objects that are not facing directly towards the camera. We would
do that same process as before, first using prelabelled data to prove the concept, then adding in the object detection
model to identify objects and their bounding boxes and orientations.

We plan to first try a Multi Hidden Layer Neural Network for our model as similar projects such as
Disnet\cite{disnet2018}. This could be expanded to be part of the YOLO CNN itself, by adding additional
output layers to predict distance and orientation in addition to the bounding box coordinates and class\cite{kiefer2023approximate}.

Some libraries we plan to use include:
\begin{itemize}
  \item PyTorch or Tensorflow for building and training our models
  \item Pretrained YOLO models for object detection (ultralytics library)
\end{itemize}

% The general plan is as follows:
% - Create a model to esimate the distances given bounding box coordinates and object classes
% - Connect this model to a YOLO object detection pipeline
% - Add in more features that describe the 3d orientation of detected objects
% - Use YOLO to determine these 3d bounding boxes

\section{Implementation}

Describe your model and implementation here. Refer to item 4. This may take around a page.

\section{Results and Evaluation}

How are you evaluating your model? What results do you have so far? What are your baselines? Refer to item 5. This may take around 0.5 pages.

\section{Feedback and Plans}

Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.

\section{Template Notes}

You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

\subsection{Tables and figures}

See Table~\ref{citation-guide} for an example of a table and its caption.
See Figure~\ref{fig:experiments} for an example of a figure and its caption.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{example-image-golden}
  \caption{A figure with a caption that runs for more than one line.
    Example image is usually available through the \texttt{mwe} package
    without even mentioning it in the preamble.}
  \label{fig:experiments}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
  \includegraphics[width=0.48\linewidth]{example-image-b}
  \caption {A minimal working example to demonstrate how to place
    two images side-by-side.}
\end{figure*}


\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.
This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
