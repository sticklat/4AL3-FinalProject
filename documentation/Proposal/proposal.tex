\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{url}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 64's Project Proposal:\\My Group's Project Name: BoxBoxBox}


\author{Thomas Stickland, Oliver (Chengze) Zhao, Larry (Yicheng) Liu \\
  \texttt{\{sticklat,zhaoc59,liu854\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Overview}

Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.

\section{Task Title and Overview}

Task title and overview, including the significance and what makes it challenging

\section{Dataset}

For this project, we will use the KITTI Depth Prediction dataset, part of the publicly availableK
ITTI Vision Benchmark Suite (\url{https://www.cvlibs.net/datasets/kitti/eval_depth_all.php}). The dataset 
was collected by the Karlsruhe Institute of Technology and the Toyota Technological Institute using 
a stereo camera system and a Velodyne HDL-64E LiDAR sensor mounted on a moving vehicle. Each sample 
in the dataset contains a RGB image captured by the cameras and a corresponding dense depth map 
generated from LiDAR measurements. The RGB image serves as the input (X) for our model, while the 
depth map acts as the label (Y), representing the ground-truth distance between each pixel in the 
image and the camera. These depth maps are provided as part of the dataset and were automatically 
generated by projecting LiDAR 3D points into the 2D camera image plane, eliminating the need for 
manual labeling.

Since the KITTI dataset is open-source and publicly available, no scraping or API access is required, 
and all data usage complies with KITTI's academic research license and terms of service. The dataset 
also includes metadata such as sensor calibration parameters, camera poses, timestamps, and 
environmental context, which enhance reproducibility and model alignment. As the full dataset consists 
of over 93,000 stereo image pairs, we plan to use a 10-20\% subset to reduce computational load while 
maintaining coverage across diverse environments (urban, rural, and highway). The dataset's high-quality 
annotations and real-world depth data make it ideal for supervised training of our vision-based obstacle 
detection and distance estimation model for autonomous drones. Unlike datasets without labels—which 
require manual annotation of objects or distances—KITTI provides pre-labeled, LiDAR-verified ground-truth 
depth values, ensuring accuracy and efficiency in the training process.

\section{Proposed Solution}
We plan to solve this problem by using a combination of object detection and regression models. However we will
first begin with prelabelled images to remove the object detection work, then once we have proven that our model
works with this prelabelled data we can add the object detection into our pipeline. For this we would use an
object detection CNN like YOLO11 to identify objects in the image and their bounding boxes, then use our model
to estimate the distance to each object based on the bounding box coordinates and the object's class\cite{Franke_Gopinath_Reddy_Ristic-Durrant_Michels_2021}.
If sucessful, this will allow us to estimate the distance to multiple objects in an image.

To build upon this first step and further improve the accuracy of our model we could add orientation data to
our feature set. This would allow us to account for the 3d orientation of objects in the image, which would
help us to better estimate the distance to objects that are not facing directly towards the camera. We would
do that same process as before, first using prelabelled data to prove the concept, then adding in the object detection
model to identify objects and their bounding boxes and orientations.

We plan to first try a Multi Hidden Layer Neural Network for our model as similar projects such as
Disnet\cite{disnet2018}. This could be expanded to be part of the YOLO CNN itself, by adding additional
output layers to predict distance and orientation in addition to the bounding box coordinates and class\cite{kiefer2023approximate}.

Some libraries we plan to use include:
\begin{itemize}
  \item PyTorch or Tensorflow for building and training our models
  \item Pretrained YOLO models for object detection (ultralytics library)
\end{itemize}

% The general plan is as follows:
% - Create a model to esimate the distances given bounding box coordinates and object classes
% - Connect this model to a YOLO object detection pipeline
% - Add in more features that describe the 3d orientation of detected objects
% - Use YOLO to determine these 3d bounding boxes


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
