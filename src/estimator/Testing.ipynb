{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f395e1c0",
   "metadata": {},
   "source": [
    "# Step 1: Parse input data into readable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b343fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_vehicle_annotations(folder_path, save_encoded=False, filepath=None):\n",
    "    \"\"\"\n",
    "    Reads KITTI-style label text files and extracts:\n",
    "    xmin, xmax, ymin, ymax, label, and distance (z in camera coordinates).\n",
    "\n",
    "    Returns both a DataFrame and PyTorch tensors with one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # Collect all .txt files in numerical order\n",
    "    files = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')])\n",
    "\n",
    "    for file in files:\n",
    "        file_id = os.path.splitext(file)[0]  # e.g. \"000000\"\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if not parts:\n",
    "                    continue\n",
    "\n",
    "                label = parts[0]\n",
    "                if label == 'DontCare':\n",
    "                    continue  # skip unlabeled/ignored regions\n",
    "\n",
    "                # bounding box coords\n",
    "                xmin = float(parts[4])\n",
    "                ymin = float(parts[5])\n",
    "                xmax = float(parts[6])\n",
    "                ymax = float(parts[7])\n",
    "\n",
    "                # z-coordinate = distance from camera\n",
    "                distance = float(parts[13])\n",
    "\n",
    "                records.append({\n",
    "                    'file_id': file_id,\n",
    "                    'label': label,\n",
    "                    'xmin': xmin,\n",
    "                    'ymin': ymin,\n",
    "                    'xmax': xmax,\n",
    "                    'ymax': ymax,\n",
    "                    'distance': distance\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # --- One-hot encode labels ---\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    df['label_id'] = df['label'].map(label_to_idx)\n",
    "    \n",
    "    # Convert to integer indices first\n",
    "    label_indices = torch.tensor(df['label_id'].values, dtype=torch.long)\n",
    "    # Convert to one-hot encoding\n",
    "    num_classes = len(unique_labels)\n",
    "    labels_onehot = F.one_hot(label_indices, num_classes=num_classes).float()\n",
    "\n",
    "    # --- Convert features to PyTorch tensors ---\n",
    "    features = torch.tensor(\n",
    "        df[['xmin', 'xmax', 'ymin', 'ymax', 'distance']].values, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    if save_encoded:\n",
    "        if filepath == None:\n",
    "            torch.save({\n",
    "                \"features\": features,\n",
    "                \"labels\": labels_onehot,\n",
    "                \"label_map\": label_to_idx\n",
    "            }, os.path.join(folder_path, \"vehicle_dataset.pt\"))\n",
    "            print(f\"Saved encoded dataset to {folder_path}/vehicle_dataset.pt\")\n",
    "        else:\n",
    "            torch.save({\n",
    "                \"features\": features,\n",
    "                \"labels\": labels_onehot,\n",
    "                \"label_map\": label_to_idx\n",
    "            }, filepath)\n",
    "            print(f\"Saved encoded dataset to {filepath}\")\n",
    "\n",
    "    return df, features, labels_onehot, label_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c004e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoded dataset to dataset/vehicle_dataset.pt\n",
      "  file_id       label    xmin    ymin    xmax    ymax  distance  label_id\n",
      "0  000000  Pedestrian  712.40  143.00  810.73  307.92      8.41         3\n",
      "1  000001       Truck  599.41  156.40  629.75  189.25     69.44         6\n",
      "2  000001         Car  387.63  181.54  423.81  203.12     58.49         0\n",
      "3  000001     Cyclist  676.60  163.95  688.98  193.93     45.84         1\n",
      "4  000002        Misc  804.79  167.34  995.43  327.94      8.55         2\n",
      "{'Car': 0, 'Cyclist': 1, 'Misc': 2, 'Pedestrian': 3, 'Person_sitting': 4, 'Tram': 5, 'Truck': 6, 'Van': 7}\n",
      "torch.Size([40570, 5]) torch.Size([40570, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df, X, y, label_map = load_vehicle_annotations(\"dataset/training/label_2\", save_encoded=True, filepath=\"dataset/vehicle_dataset.pt\")\n",
    "\n",
    "print(df.head())\n",
    "print(label_map)\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fdaf1",
   "metadata": {},
   "source": [
    "## Basic NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beebfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistanceRegressorOneHot(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        \"\"\"\n",
    "        input_dim: total number of features including one-hot label\n",
    "        \"\"\"\n",
    "        super(DistanceRegressorOneHot, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # output: distance\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Helper function to one-hot encode labels ---\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    labels: torch tensor of shape [N] with integer label IDs\n",
    "    returns: tensor of shape [N, num_classes] with one-hot encoding\n",
    "    \"\"\"\n",
    "    return torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "# --- Training function ---\n",
    "def train_distance_model_onehot(features, labels, num_classes, epochs=50, batch_size=64, lr=1e-3):\n",
    "    \"\"\"\n",
    "    features: tensor of shape [N, 4] -> xmin, xmax, ymin, ymax\n",
    "    labels: tensor of shape [N] -> label_id as integer\n",
    "    num_classes: total number of vehicle types\n",
    "    \"\"\"\n",
    "    # One-hot encode labels\n",
    "    labels_onehot = one_hot_encode(labels, num_classes)\n",
    "    \n",
    "    # Concatenate bbox features + one-hot label\n",
    "    X = torch.cat([features, labels_onehot], dim=1)\n",
    "    y = labels.view(-1, 1)  # distance targets\n",
    "\n",
    "    # Dataset and split\n",
    "    dataset = TensorDataset(X, y)\n",
    "    n_train = int(0.8 * len(dataset))\n",
    "    n_val = len(dataset) - n_train\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = DistanceRegressorOneHot(input_dim=X.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = model(X_batch)\n",
    "                val_loss += criterion(preds, y_batch).item() * X_batch.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
